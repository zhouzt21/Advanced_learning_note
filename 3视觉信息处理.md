# 视觉信息处理

Vision information processing

资料来源：视听导课程

0. 视觉信息处理与编码
   0.1 线性平移不变的图像滤波
   0.2 用于模板匹配的滤波
   0.3 下采样和图像金字塔
   0.4 图像压缩技术
       预测编码
       变换编码
       统计编码

1. 视觉特征
   1.1 边缘检测
   1.2 角点检测
   1.3 光流估计

2.  图像分类
   2.1 传统方法:词袋
   2.2 深度学习:神经网络部分略(见媒认笔记)

3. 目标检测

   3.1 传统方法:DPM
   3.2 基于cnn的方法:RCNN家族和R-FCN

## 0 视觉信息处理与编码

### 0.1 线性平移不变的图像滤波 

0. 盒滤波器/方框均值滤波器
   * 可拆分的滤波器：一个2维可拆分的滤波器可以被写作column和row的乘积

1. sober filter
   * 思想：有限差分

     * 1维导数滤波器：（1 0 -1）
     * 

   * vertical sobel：留下水平边缘

   * horizontal sobel: 留下垂直边缘

   * ![image-20240113152931303](.\asset\sobel.png)
     $$
     水平滤波器：\frac{\partial f}{\partial x} = S_x * f ； 垂直滤波器： \frac{\partial f}{\partial y} = S_y * f \\
     $$
   
2. 高斯滤波器

   * 权重值由中心向两边衰减 ；在理论上，高斯滤波器的卷积核是无限大的，但在实际应用中，通常会将它截断到某个最大距离
   * 问题牵引：
     * 如何在图像中检测边缘——求导数；
     * 在离散图像求导——**有限差分**
     * 使用梯度滤波器之前需要先**平滑**

   * **高斯滤波器的导数**（derivative of Gaussian，DoG）

     * $$
       卷积中的导数定理: \frac{\partial}{\partial x}{(h*f)} = (\frac{\partial}{\partial x}h)*f
       $$

     * 也即原本先平滑再求一阶导，变成平滑和求导统一成一个算子DoG再作用于图像。

     * ![image-20231108163447654](.\asset\20231108_derivative_guassian.png)

   *  **高斯-拉普拉斯滤波**  laplacian of guassian (LoG)

     * 拉普拉斯算子是一个二阶算子![image-20231108163622455](.\asset\20231108_laplace.png)

     * LoG 是求二阶导和高斯算子的结合![image-20231108163537639](.\asset\20231108_LoG.png)

       特征是在边缘处过零点。Zero-crossing方法更准确地定位边缘，但不是很方便。

![image-20240113152544753](.\asset\zero_crossing.png)

![image-20240113152636001](.\asset\guassian.png)

这里DoG难道不应该是u v方向上的一阶导嘛，为什么对x求导？

### 0.2 用于模板匹配的滤波

解决方案1：使用模板作为滤波器核来滤波图像（几乎不行）
$$
ℎ[𝑚, 𝑛] = \sum_{𝑘,𝑙} 𝑔[𝑘, 𝑙]𝑓[m+k, n+l]
$$
解决方案2：使用零均值模板滤波图像
$$
ℎ[𝑚, 𝑛] = \sum_{𝑘,𝑙} (𝑔[𝑘, 𝑙]-\overline g)𝑓[m+k, n+l]
$$

* 对高对比度的区域不稳定

解决方案3：使用差异的平方和（SSD）
$$
ℎ[𝑚, 𝑛] = \sum_{𝑘,𝑙} (𝑔[𝑘, 𝑙]-𝑓[m+k, n+l])^2
$$
解决方案4：归一化互相关（NCC）
$$
ℎ[𝑚, 𝑛] = \frac{\sum_{𝑘,𝑙} (𝑔[𝑘, 𝑙]-\overline g)(𝑓[m+k, n+l]-\overline{f_{m,n}})}{\sqrt{\sum_{𝑘,𝑙} (𝑔[𝑘, 𝑙]-\overline g)^2\sum_{𝑘,𝑙}(𝑓[m+k, n+l]-\overline{f_{m,n}})^2}}
$$


选择何种方式取决于关心速度还是关心不变性：

*  零均值：最快，对局部强度非常敏感。
*  差异平方和：中速，对强度偏移敏感。
* 归一化互相关：最慢，对对比度和亮度不变

### 0.3 下采样和图像金字塔

朴素的图像下采样

*  这种效应叫混叠
  * 混叠：下采样可以将一个信号伪装成一个较低频率的信号，我们总是可以将信号与一个频率更高的信号混淆

* 如何处理混叠
  * 方法1：对信号进行上采样 
    * 这就是相机制造商如此关注提高像素分辨率的原因。
  * 方法2：平滑信号
    * 消除一些导致混叠的高频效应。
    * 丢失信息，但比混叠效果好。

* 抗混叠
  * 奈奎斯特定理：如果系统以大于该信号最大频率的两倍的速率对模拟信号进行均匀采样，则可以从该采样产生的离散值中完全恢复原始模拟信号
  * 更好的解决方案：先用平滑滤波器再删去一半的行和列

* 高斯金字塔
  * 算法：重复**下采样-滤波**，直到到达最小分辨率，形成高斯金字塔
  * 图像的细节：转移到更高的层次时，它们会变得平滑
  * 更高的层次上保存了对应于原始图像中较大的均匀区域
  * 不可能从低分辨率图像重建原始图像
  * 模糊是有损的

* 拉普拉斯金字塔
  * 构造算法：重复 **滤波-计算残差-下采样**，直到最小分辨率，形成拉普拉斯金字塔
  * 重建算法：重复**上采样-和残差相加**，直到到达原始分辨率，还原图像
  * 在每一层上保留残差，而不是模糊的图像本身，则可以重建图像

### 0.4 图像压缩技术

使用视觉冗余来实现有损压缩，而不损失视觉感知。

* 可以被压缩的地方
  * 时间冗余：视频图像中相邻帧之间的相似性
  * 空间冗余：相邻像素之间的强相关性
  * 编码冗余：不同像素值出现的不同概率
  * 视觉冗余：人眼对某些细节不敏感
  * 知识冗余：可以从先验知识中获得

* 无损失的
  * X = X’
  * 低压缩比（1/2 ~ 1/3）
  * Winzip, JPEG-LS

* 有损的
  *  X != X’
  * 高压缩比（1/10 ~ 1/20）
  * MPEG-2

#### 预测编码

* 去除像素序列之间的相关性需要了解源数据的**概率特征**；这项技术涉及基于先前编码的像素来预测一个像素的值，并利用相邻像素之间的相关性。
  * 预测编码可以分为差分编码和可变长度编码两类。差分编码计算像素的预测值与实际值之间的差异，然后可以使用可变长度编码对这些差异进行编码，以减少比特率。

* **I帧**代表"帧内帧"（intra-frame），也称为关键帧（key frame）、独立编解码帧（independent coding-decoding frame）或关键图像帧（key image frame）。
  * I帧不依赖于其他帧，可以被独立解码成一幅完整的图像。
    * 例如，在视频传输中，每隔一段时间发送一个I帧以确保视频质量，而P帧和B帧可以与前一个I帧一起显示一段时间。这是一种常见的视频编码和传输技术，通常用于提高效率和减少带宽占用。

* **P帧**（预测帧），它们通过运动估计和补偿技术来预测**当前帧与前一个I或P帧**之间的差异，以进行压缩编码，从而减小视频文件大小并提高编解码效率。
* **B帧**（双向预测帧），它们是使用**过去的一个I或P帧**和**未来的一个P或I帧**进行编码的。这进一步提高了编码的效率，但也增加了编码和解码的复杂性。B帧可以从两个方向进行参考，因此可以更好地捕捉运动和差异，但同时也需要更多的计算资源来进行编解码。B帧通常在视频编码中用于提高压缩效率和视频质量。

**运动补偿**

(没有理解，运动补偿和运动预测是另一种没有被介绍的方法嘛？)

* 运动补偿涉及研究视频中的物体或主体的运动，并预测它们将在未来帧中的位置。
* 运动补偿的过程是通过将帧分成一组称为宏块（macroblocks）的小区域来完成的。然后，对每个宏块进行分析，以确定其运动以及在预测帧中应如何进行补偿。
* 图像块匹配：

$$
最大均方差：𝑀𝑆𝐸(𝑖,𝑗) =\frac{1}{𝑁^2}\sum_{𝑥=1}^𝑁\sum_{𝑦=1}^𝑁(𝐼_𝑒(𝑥,𝑦) − 𝐼_𝑟(𝑥 + 𝑖, 𝑦 + 𝑗))^2\\
最小平均绝对差：𝑀𝐴𝐷(𝑖,𝑗) =\frac{1}{𝑁^2}\sum_{𝑥=1}^𝑁\sum_{𝑦=1}^𝑁|𝐼_𝑒(𝑥, 𝑦)− 𝐼_𝑟(𝑥 + 𝑖, 𝑦 + 𝑗)|
$$

* 一旦确定了每个宏块的运动，编码器可以生成一个考虑了视频中物体的运动的预测帧。

* 然后，将这个预测与实际帧进行比较，然后使用诸如变换编码（transform coding）和熵编码等技术对预测帧和实际帧之间的差异进行编码，进一步减少需要编码的数据量。

#### 变换编码

涉及将图像分解成其频率成分，通过应用数学变换，如**离散余弦变换（DCT）**、小波变换或离散傅立叶变换。然后对变换后的系数进行**量化和编码**。

* **原因**：能量在空间域中更分散，但在变换域中更集中。因此，在变换域中为能量较低的区域分配较少或不分配编码词更为方便

* **目的：** 移除空间相关性；将空间信号的能量集中在频率域中的少量低频系数上，通过量化来去除小系数，而不影响重建图像的质量

* DC系数用多位表示，而AC系数用较少的位表示。此外，在AC系数矩阵中有许多零
  * 将绝对值小于40的DCT系数设为零；所有的系数都是四舍五入到整数； 使用了一个量化矩阵

#### 统计编码

根据符号出现的**统计概率**来对符号进行编码。

* 霍夫曼编码是一种常用的技术。频繁出现的符号被分配较短的编码，而不经常出现的符号则被分配较长的编码。这种方式可以有效地减小数据的表示大小，特别适用于数据中存在不同符号的概率分布不均匀的情况。

* 其基本思想是用较短的码字表示出现概率高的符号，用较长的码字表示出现概率低的符号，从而使**平均码长接近信息熵**。

$$
源信息熵：𝐻(𝑠) = −\sum_{𝑖=1}^𝑛 𝑞_𝑖log_2 𝑞_𝑖\\
其中 𝑞𝑖 是符号 𝑠𝑖 被源数据发送的可能性。\\
平均代码长度：\overline L =\sum_{𝑖=1}^𝑛 𝑙_𝑖𝑞_𝑖\\
编码效率： \eta = \frac{𝐻(𝑠)}{\overline L}\\
压缩比 ：k=\frac{自然二进制码长}{\overline L}\\
码方差： \sigma^2=E((l_i - \overline L)^2)\\
冗余度： r = 1- \eta
$$

1. 计算每个符号中出现的次数。

2. 根据符号出现的频率，按升序排序。

3. 出现频率最小的两个符号，构建一个二叉树，以这两个符号作为左右叶节点。
4. 将新生成的节点放回计数表中并重新排序
5. 重复步骤3和步骤4，直到所有的节点都被构造成一个二叉树。
6. 对于树中的每个叶节点，从根到叶节点的路径表示该符号的代码。左子路径为0，右子路径为1
7. 连接所有的符号代码，以获得最终的霍夫曼代码

## 1 视觉特征

### 1.1 边缘检测

**目标**：识别图像中的突变（不连续性）

直观地看，边缘携带了图像的大部分语义和形状信息

* **非极大值抑制(NMS)**  ：对于每个梯度范数高于阈值的位置q，检查梯度是否高于沿梯度方向的邻近位置p和r
* **滞后阈值**：使用高阈值来开始边缘曲线，然后使用低阈值来延续它们

### 1.2 角点检测

**基本思想**

• 我们能通过一个小窗口轻松识别角点

• 以任何方向移动窗口都会引起强烈的亮度变化

![image-20231209114328979](.\asset\视觉_角点.png)

![image-20231209114436235](.\asset\视觉_角点2.png)

* 中间的矩阵是自相关矩阵，自相关矩阵是一个可以一次性计算所有像素的“局部算子”

  * 任意的 N×N **实对称矩阵**都有 N 个线性无关的特征向量。并且这些特征向量都可以正交单位化，而得到一组**正交且模为** **1** 的向量  QAQ_T
  * Q矩阵代表了对(u,v)的坐标变换，变换到(𝒗𝟏 𝒗𝟐)为轴的坐标系中，保持幅度不变，特征值 *λ*1 ,*λ*2 代表了沿两个方向E(u,v)函数变化幅度的大小。
  * 两个特征值都大代表角点，一大一小代表边缘；都小代表没有强烈变化（因为自相关矩阵的特征向量的大小代表这个方向上的变化/相似性大小）

Harris角点检测：

![image-20231209114849851](.\asset\视觉_harris.png)

* 不变性是指：图像变换后，角点位置不变

* 协变性是指: 如果两张同一图像的变换版本，则在相应的位置检测到相同特证

Harris角点的不变性和协变性:

* 对于强度变化（亮度和对比度的仿射变换）是**不变的**，因为我们使用导数
* 相对于平移是协变的，因为导数和卷积是平移不变的
* 相对于旋转是协变的，因为椭圆特征值将保持不变
*  **角点相对于缩放不是协变的**，因为卷积具有单一尺度

### 1.3 光流估计

* **定义**: 光流是图像中亮度模式的表观运动，理想情况下，亮度情况和运动场相同

* 注意表观运动可能是由于亮度变化导致的，而可能实际没有运动
* 估计光流的假设
  * 亮度不变性：同一点的投影再每一帧中看起来相同
  * 微小移动：点的移动不会很远
  * 空间一致性：点的移动与其相临近的点类似

![image-20231209115355394](.\asset\视觉_光流估计.png)

![image-20231209115650123](.\asset\视觉_光流估计最小二乘.png)

问题解决：

* 运动较大（大于一个像素)
  * 多分辨率估计，迭代精化
  * 特征匹配
* 某点的运动方式与其周围不同
  *  运动分割

最新：使用机器学习推断密集的光流场

## 2 图像分类

* 评价指标：精确度和召回率
  * 精确度是指模型正确预测与模型总预测之比。
  * 召回率是我们的模型正确预测与实际标签之比。
  * ![image-20240113200855607](.\asset\评价指标.png)
  * 准确率：（是针对整个数据集预测而言的）![image-20240113201126609](.\asset\准确率.png)
  * 召回率：（是针对正例的预测而言的）![image-20240113201208887](.\asset\召回率.png)

#### 2.1 传统方法：词袋

方法：

* 从训练数据中创建一个字典（视觉词汇是从**训练集**中学习的）
  * 从训练数据中**提取局部特征描述向量**
  * 划分特征描述向量的空间
  * 定义**视觉单词**
    * 通常采无监督的聚类算法得到，如k-means（k是指k个聚类中心）
    * 相当于给图像建立了一套 ”线性空间的坐标体系“

* 将测试图像转化为词频直方图
  * **提取局部特征描述向量**
    * 以HOG（梯度直方图）为例子
    * 直方图包含 9 个“桶”，对应的角度为 0、20、40... 160。
    * 根据梯度方向选择一个或两个 “桶” ，将梯度幅度值按照梯度方向的比重加入选择的“桶”
  * 为每个描述符**分配视觉单词**（根据字典）
    * 用例如最近邻的方法分配视觉单词
  * 生成**词频直方图**
    * 此时已经让测试图像拥有了这套坐标体系

* 使用词频直方图进行分类
  * 利用线性分类器
    * （我认为这里的线性分类器应该是还要通过有监督学习训练出来，前面只是建立一套“坐标体系”）

#### 2.2 深度学习：神经网络部分略（见媒认笔记）

## 3 目标检测

#### 3.1 传统方法：DPM

**Dalal & Triggs检测器**

* 他们发现方向梯度（**histograms of oriented gradient** HOG）描述符的直方图网格在人类检测方面明显优于现有的特征集（其对光照和对比度的变化具有鲁棒性

* 以多种分辨率计算整个图像的HOG，形成特征图金字塔；为特征金字塔的每个窗口打分；并应用非极大抑制，选择最终的候选框

* **非极大抑制（NMS）**

  * 问题牵引：检测后，我们在不同的单元格中得到不同的分数，选择哪个单元格作为边界框
    * 不能一味调整 整体阈值：不同的对象可能有不同的分数高低，改变阈值会导致在一个对象中出现多个边界框、或者一个对象中没有边界框
    * 应该在选出较大置信度的框中，删除相互重叠的方框（**高IoU值**的框）

  * 同时召回率不算高
    * HOG检测器将一个对象类建模为一个单一的固定模板，但是物体是由部分组成的，甚至整体物体也是由零件组成的
    * 解决办法：物体是由可变形的组件组成的

**基于可变形部件的模型 Deformable Part-based Model (DPM)**

* 一系列组件（零件/部件）模板按照可变的方式排列
* 每个模型都有全局模板+零件模板
* 通过原有边界框即可训练（不需要零件的边界框）

$$
𝑆𝑐𝑜𝑟𝑒(𝑝_0, … , 𝑝_1) = \sum_{𝑖=0}^𝑛 𝑤_𝑖⋅𝜙(𝐼,𝑝_𝑖)−\sum_{𝑖=0}^𝑛 𝑑_𝑖⋅𝜙_𝑑(𝑑𝑥_𝑖, 𝑑𝑦_𝑖) + 𝑏\\
各个部件的候选位置p_i，各个部件的过滤器w_i,位置 p_i 处子窗口的特征向量𝜙_i\\
变形参数(4维向量)初始化:𝑑_𝑖 = (0,0,1,1)\\
第i部分相对于其锚定点（根）的位置的位移函数:𝜙_𝑑(𝑑𝑥, 𝑑𝑦) = (𝑑𝑥, 𝑑𝑦, 𝑑𝑥^2, 𝑑𝑦^2)\\
根据各个部件的最佳位置计算根位置的总得分(这里以p_0为例)：\\
𝑠𝑐𝑜𝑟𝑒(p_0) =max_{p_1,…,p_n}𝑠𝑐𝑜𝑟𝑒(𝑝_0, 𝑝_1, … , 𝑝_𝑛)
$$

* 高评分的根位置确定了检测结果
* 通过滑动窗口方法检测
* 高效计算（O（nk））：动态规划+广义距离变换

距离变换：
$$
特征金字塔第l层的第i部分过滤器的响应分数：\\
𝑅_{𝑖,𝑙}(𝑥,𝑦) = 𝑤_𝑖 ⋅ 𝜙(𝐼, 𝑥, 𝑦, 𝑙 )\\
计算若根在(x, y)处，第i部分的最佳分数大小：（相似程度和相对距离的权衡取舍）\\
𝐷_{𝑖,𝑙}(𝑥,𝑦) =  max_{𝑑𝑥,𝑑𝑦} (𝑅_{𝑖,𝑙}(𝑥 + 𝑑𝑥, 𝑦 + 𝑑𝑦) − 𝑑_𝑖 ⋅ 𝜙_𝑑(𝑑𝑥, 𝑑𝑦))
$$

* 总体理念：扩充模型内容以强化对物体的表示

#### 3.2 基于cnn的方法： RCNN家族和R-FCN 

 **（可以结合媒认笔记）**

滑动窗口不可接受的计算成本，解决方案：

* 改进方法1：启发式算法（RCNN，Fast RCNN）
  * 例如，选择性搜索（Selective Search）用于生成候选框（proposal boxes）。但这种方法依赖外部算法，使系统实现变得复杂，难以进行性能的联合优化。

* 改进方法2：使用CNNs进行预测候选框（Faster RCNN，……）
  * 这是目前一种广泛使用的方法

**区域候选（Region-Proposal）**

* **基于区域的CNN（RCNN,2013）**

  * 阶段1 ：生成候选框（proposal boxes） 
    * 使用传统的算法，大约有2000个框被选择可能包含物体

  * 阶段2 ： 识别候选框（proposal boxes）
    * 将候选框内的图像缩放到固定的大小，并输入卷积网络进行进一步识别，以获得准确的结果。

* **消除冗余计算：Fast R-CNN（2014）**
  * 阶段1同上
  * 阶段2：
    * 对整个图像应用卷积层，一次计算所有位置的图像特征（重叠部分并不重复计算）
    *  与候选框对应的特征图被裁剪并送到全连接层进行分类

* **插入区域候选网络RPN：Faster R-CNN**

  * 让CNN来选候选框：
    * 假设在特征图中的每个点上都有一个固定大小的锚定框
    * 在每一点上，预测相应的锚点是否包含一个对象
      * Perform binary classification（二分类） and bounding box regression（边界框回归） for each anchor box
    * 在实践中，在每个点上使用K个不同大小/规模的锚框

  * 联合训练Loss函数的四个部分：
    * RPN的 对象/非对象 分类Loss
    * RPN回归框坐标Loss
      * 用回归的方式去微调候选框，使其更准确地框取物体，这过程我们称bounding box regression
    * 最终分类分数（对象类别）Loss
    * 最终方框坐标Loss
      * 

* **基于区域的全卷积网络（R-FCN）**

  结合全局语境建模和位置感知的全卷积网络

  R-FCN比Faster R-CNN更快

  * 提高图像分类的平移不变性
  * R-FCN通过减少每个RoI所需的工作量来提高速度（使用全连接网络）
  * 基于区域的特征图独立于RoI，可以在每个RoI之外计算

* 提高图像分类的平移不变性
  * 一个物体在图像内部的移动应该是不加区分的
  * 良好的深度（完全）卷积架构是平移不变的

* 目标检测中的平移协变性
  * 响应应该反映候选方框与对象重叠程度



RFCN不懂
