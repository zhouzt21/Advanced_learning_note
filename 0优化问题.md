# 优化问题

资料来源：视听导课程

1. 问题的定义
2. 优化算法
   *2.1 按照阶数划分算法
       零阶算法一阶算法
        二阶算法
   2.2 无约束最小化
        普通梯度+线性步长搜索
         牛顿梯度下降

3. 深度学习中的优化
   3.1随机梯度下降:
   3.2 momentum(梯度更新+速度=实际更新方向)关于学习率
         Factor Scheduler

   ​       Multi Factor Scheduler
   ​      Cosine Scheduler

### 1问题的定义

![image-20231025151608424](.\asset\优化_不同的优化问题.png)



### 2 优化算法

#### * 2.1 按照阶数划分算法

##### 零阶算法

* Gloden Section
  * ![image-20231025145833011](.\asset\优化_golden_section.png)

#####  一阶算法

First-order optimization algorithms explicitly involve using the first derivative (gradient) to choose the direction to move in the search space.

* Gradient descent
* Stochastic Gradient Descent
* Momentum
* bisection

##### 二阶算法

Second-order optimization algorithms explicitly involve using the second derivative 

(Hessian) to choose the direction to move in the search space.

These algorithms are only appropriate for those objective functions where the Hessian 

matrix can be calculated or approximated.

#### 2.2 无约束最小化

此问题可以变成对于点集，迭代找到满足目标函数最小化的点的问题（点即解集）

![image-20231025150641384](.\asset\优化_无约束最小化.png)



#### 普通梯度+线性步长搜索：

![image-20231025150833203](.\asset\优化_普通梯度.png)

* 停止准则通常采用 ||∇𝑓(𝑥)||^2 ≤ 𝜖形式 （导数的欧式长度足够小为止

* 原始的梯度下降简单，但通常非常慢

![image-20231025151056571](.\asset\优化_梯度下降.png)

#### 牛顿梯度下降：

![image-20231025151331703](.\asset\优化_牛顿梯度下降.png)

$$
上述来自于展开： \triangledown𝑓(𝑥 + 𝑣) ≈ \triangledown \widehat𝑓 (𝑥 + 𝑣 )= \triangledown𝑓(𝑥) + \triangledown^2 𝑓 (𝑥) *𝑣 = 0 \\
\lambda(x) 是牛顿衰减。
$$


### 3 深度学习中的优化

深度学习中的优化问题非凸，所以解决起来更困难。

深度学习中优化问题会遇到的困难：

* 优化目标：比起一般的优化来说，在设计Loss function的时候要额外注意过拟合，要注意设计泛化误差和训练误差

* Saddle Points 局部最小值：
  * 判断：hessen矩阵的正定性（特征值全正或者全负能获得局部最值，但是有正有负则可能获得鞍点；在最小化的问题中，需要的就是正定，特征值均正）

* Vanishing Gradients

![image-20231025152815769](.\asset\优化_vanishing_gradient.png)

由于计算成本，上述方法不能之间用在深度学习中；常用的优化算法：

#### 3.1随机梯度下降:

* ![image-20231025153656780](.\asset\优化_SGD引入.png)

* 一般的梯度下降，使用所有的梯度取平均，而梯度下降是随机选取一个方向作为代表

* SGD优点：迭代成本与m（批次的数量）无关； 也可以在内存使用量方面大大节省开支

  #### mini-batch SGD：折中的办法是随机选一小批来算方向

* ![image-20240113133557745](.\asset\优化_min_batch.png)

* 使用小批量数据可以将方差减小到原来的 1/𝑏，但计算成本也增加了 b 倍

仍然会出现局部最小值/鞍点（高维度常见）问题，解决方法：

#### 3.2 momentum （梯度更新+速度=实际更新方向）

* ![image-20231025155122737](.\asset\优化_momentum.png)
  * ![image-20231025155146561](.\asset\优化_N_momentum.png)
  * ![image-20231025155216691](.\asset\优化_N_momentum_2.png)
  * 这里是先预测当前速度更新会到的位置，然后**该位置**计算梯度，将**此梯度**和当前速度结合起来确定实际的更新方向。这样能有效前瞻——预估未来位置梯度（而不是当前参数的梯度）

  #### 3.3 adagrad （自适应学习率）

  根据参数自适应地调整学习率（为了解决问题：在学习率减小的时候，常见特征的参数相对块地收敛到最优值，而不常见的则尚未足够优化）

  ![image-20231025155633509](.\asset\优化_adamgrad.png)

  * 也即梯度大的学习率大，梯度小的学习率大，进行一个自适应。（通过协方差的调整实现）
  * 经过多轮迭代之后学习率会减小到0，不能得到任何更新。

  #### 3.4 rmsprop 

  ![image-20240113143545830](.\asset\优化_rms.png)

  * 在adagrad的基础上，给协方差的过去值和更新的梯度之间分配合适的比例

  * 另外常数 𝜖 > 0 通常被设置为 1 × 10 −6 ，以确保不会因为零除或过大的步长而遇到问题。

  #### 3.4 adam（自适应矩估计——结合以上所有方法）

  * ![image-20231025161646551](.\asset\优化_adam_1.png)
    * 第一矩：均值v
    * 第二矩：非中心化的协方差s
  * ![image-20231025161623422](.\asset\优化_adam.png)

  要阅读更多内容，请访问：

  • https://www.ruder.io/optimizing-gradient-descent/#visualizationofalgorithms

  • https://d2l.ai/chapter_optimization/

  • https://distill.pub/2017/momentum/

#### 关于学习率

* 如果学习率太大，优化就会发散，如果太小，它就需要太长的时间来训练，否则最终会得到一个次优的结果
* 如果学习率较大，最初可以快速优化，但最终可能就会在最小值附近震荡，因此无法达到最优状态
* 开始时的大学习率可能没有好处，特别是由于初始参数集是随机的。最初的更新方向可能也相当毫无意义

##### Factor Scheduler

* 一个替代多项式衰减的方法是乘法衰减，即𝜂^{𝑘+1} = 𝜂^𝑘 ⋅ 𝛼, for 𝛼 ∈(0,1)。
  * 为了防止学习率衰减到一个合理的下限以下，更新方程常常被修改为𝜂^(𝑘+1) = max(𝜂_𝑚𝑖𝑛, 𝜂^𝑘 * 𝛼)。

##### Multi Factor Scheduler

* 在深度网络训练中，一个常见的策略是分段学习率，每隔一段时间就减少一定的比例。
* 给定一个时间集合来决定何时降低学习率，比如𝑠 = {5, 10, 20}，𝜂 𝑘+1 = 𝜂 𝑘 ⋅ 𝛼 𝑤ℎ𝑒𝑛𝑒𝑣𝑒𝑟 𝑘 ∈ 𝑠。

##### Cosine Scheduler

$$
𝜂^{𝑡+1} = 𝜂^{𝑇} + \frac{𝜂^0 − 𝜂^𝑇}{2}(1 + cos(\frac{𝜋𝑡}{𝑇}) )\\
这里𝜂^0是初始学习率，𝜂^𝑇是当𝑇时的目标学习率。\\
此外，对于𝑡 > 𝑇，我们只需将值固定到𝜂^𝑇而不再增加它。
$$

