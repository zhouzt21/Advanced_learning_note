# 深度学习概论（略）

注： 神经网络部分直接查看媒体与认知笔记

资料来源：视听导课程

### 1 MLP

* 激活函数：
  * tanh 𝑥 = (1−exp(−2𝑥))/(1+exp(−2𝑥))
  * ReLU：当前最常使用的激活函数是修正线性单元（Rectified linear unit，ReLU）， 它实现简单，同时在各种预测任务中表现良好。 ReLU提供了一种非常简单的非线性变换。 给定元素x，ReLU函数被定义为该元素与0的最大值。

* 损失函数
  * ![image-20240112161413630](.\asset\神经网络里的损失函数.png)

### 2 CNN

* 正则化
  * ![image-20231025162502840](.\asset\DL_regularization.png)

* dropout
* batch normalization
  * ![image-20231025164021562](.\asset\DL_batch_normalization.png)
  * 通常在全连接层后，在非线性层之前
* normalization
  * ![image-20231025164509230](.\asset\DL_normalization_1.png)
  * ![image-20231025164616668](.\asset\DL_normalization_2.png)
* stochastic depth ,  fractional pooling

重点记住以下结论：

![image-20231025163609772](.\asset\DL_CNN.png)

![image-20231025163741968](.\asset\DL_CNN_filter.png)

* pooling
  * ![image-20231025163939146](.\asset\DL_pooling.png)

---

### 3 RNN

A class of neural networks allowing to handle variable length inputs.

![image-20231025164806640](.\asset\DL_RNN.png)



----

基础代码训练：coding1

* 关于过拟合

loss下降的同时accuracy下降表示进入overfitting的领域，可以想象成你的模型在训练中依然进步，但是在validation set上测出的accuracy已经开始下降了。

接下来，accuracy反升，这表示你的模型中的regularization开始起作用，在成功遏制overfitting，于是你的accuracy开始回升，但是作为代价你的loss会退步，因为你的模型在阻止自己过度拟合训练数据。

简单来说，双降表示你的模型在训练数据中一头扎得太深，双升表示你的模型意识到这个问题，开始回过头来补救。

https://www.zhihu.com/question/347497524/answer/833652981




----

技术管理：
《跨越鸿沟》

