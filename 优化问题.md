## 优化问题

资料来源：视听导课程

### 1问题的定义

![image-20231025151608424](.\asset\优化_不同的优化问题.png)





### 2 优化算法

#### 2.1 按照阶数划分算法

##### 零阶算法

* Gloden Section
  * ![image-20231025145833011](.\asset\优化_golden_section.png)

#####  一阶算法

First-order optimization algorithms explicitly involve using the first derivative (gradient) to choose the direction to move in the search space.

* Gradient descent
* Stochastic Gradient Descent
* Momentum
* bisection



##### 二阶算法

Second-order optimization algorithms explicitly involve using the second derivative 

(Hessian) to choose the direction to move in the search space.

These algorithms are only appropriate for those objective functions where the Hessian 

matrix can be calculated or approximated.



* Newton’s Method



#### 2.2 无约束最小化

此问题可以变成对于点集，迭代找到满足目标函数最小化的点的问题（点即解集）

![image-20231025150641384](.\asset\优化_无约束最小化.png)

梯度算法：

普通梯度+线性步长搜索

![image-20231025150833203](.\asset\优化_普通梯度.png)

梯度下降算法：

![image-20231025151056571](.\asset\优化_梯度下降.png)



牛顿梯度下降：

![image-20231025151331703](.\asset\优化_牛顿梯度下降.png)

𝑥 + = 𝑥 − ( ∇^2 𝑓(𝑥))^ −1 * ∇𝑓(𝑥)

牛顿的办法保证二阶导为0







### 3 深度学习中的优化

Almost all optimization problems arising in deep learning are *nonconvex*. （非凸，所以解决起来更困难）

深度学习中优化问题会遇到的困难：

* 优化目标：比起一般的优化来说，在设计Loss function的时候要额外注意过拟合，要注意设计泛化误差和训练误差

* Saddle Points 局部最小值：
  * 判断：hessen矩阵的正定性（特征值全正或者全负能获得局部最值，但是有正有负则可能获得鞍点；在最小化的问题中，需要的就是正定，特征值均正）

* Vanishing Gradients

![image-20231025152815769](.\asset\优化_vanishing_gradient.png)



常用的优化算法：

* 随机梯度下降:
  * ![image-20231025153656780](.\asset\优化_SGD引入.png)
  * 一般的梯度下降，使用所有的梯度取平均，而梯度下降是随机选取一个方向作为代表
  * mini-batch SGD：折中的办法是随机选一小批来算方向

* 解决局部最小值的方法：
  * momentum
  * ![image-20231025155122737](.\asset\优化_momentum.png)
    * ![image-20231025155146561](.\asset\优化_N_momentum.png)
    * ![image-20231025155216691](.\asset\优化_N_momentum_2.png)
  * adam（Adaptive Moment Estimation）
    * ![image-20231025161646551](.\asset\优化_adam_1.png)
    * ![image-20231025161623422](.\asset\优化_adam.png)
  * adamgrad
    * ![image-20231025155633509](.\asset\优化_adamgrad.png)
  * rmsprop



Factor Scheduler???(后面这几页没懂)