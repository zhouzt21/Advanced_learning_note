## ä¼˜åŒ–é—®é¢˜

èµ„æ–™æ¥æºï¼šè§†å¬å¯¼è¯¾ç¨‹

### 1é—®é¢˜çš„å®šä¹‰

![image-20231025151608424](.\asset\ä¼˜åŒ–_ä¸åŒçš„ä¼˜åŒ–é—®é¢˜.png)





### 2 ä¼˜åŒ–ç®—æ³•

#### 2.1 æŒ‰ç…§é˜¶æ•°åˆ’åˆ†ç®—æ³•

##### é›¶é˜¶ç®—æ³•

* Gloden Section
  * ![image-20231025145833011](.\asset\ä¼˜åŒ–_golden_section.png)

#####  ä¸€é˜¶ç®—æ³•

First-order optimization algorithms explicitly involve using the first derivative (gradient) to choose the direction to move in the search space.

* Gradient descent
* Stochastic Gradient Descent
* Momentum
* bisection



##### äºŒé˜¶ç®—æ³•

Second-order optimization algorithms explicitly involve using the second derivative 

(Hessian) to choose the direction to move in the search space.

These algorithms are only appropriate for those objective functions where the Hessian 

matrix can be calculated or approximated.



* Newtonâ€™s Method



#### 2.2 æ— çº¦æŸæœ€å°åŒ–

æ­¤é—®é¢˜å¯ä»¥å˜æˆå¯¹äºç‚¹é›†ï¼Œè¿­ä»£æ‰¾åˆ°æ»¡è¶³ç›®æ ‡å‡½æ•°æœ€å°åŒ–çš„ç‚¹çš„é—®é¢˜ï¼ˆç‚¹å³è§£é›†ï¼‰

![image-20231025150641384](.\asset\ä¼˜åŒ–_æ— çº¦æŸæœ€å°åŒ–.png)

æ¢¯åº¦ç®—æ³•ï¼š

æ™®é€šæ¢¯åº¦+çº¿æ€§æ­¥é•¿æœç´¢

![image-20231025150833203](.\asset\ä¼˜åŒ–_æ™®é€šæ¢¯åº¦.png)

æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼š

![image-20231025151056571](.\asset\ä¼˜åŒ–_æ¢¯åº¦ä¸‹é™.png)



ç‰›é¡¿æ¢¯åº¦ä¸‹é™ï¼š

![image-20231025151331703](.\asset\ä¼˜åŒ–_ç‰›é¡¿æ¢¯åº¦ä¸‹é™.png)

ğ‘¥ + = ğ‘¥ âˆ’ ( âˆ‡^2 ğ‘“(ğ‘¥))^ âˆ’1 * âˆ‡ğ‘“(ğ‘¥)

ç‰›é¡¿çš„åŠæ³•ä¿è¯äºŒé˜¶å¯¼ä¸º0







### 3 æ·±åº¦å­¦ä¹ ä¸­çš„ä¼˜åŒ–

Almost all optimization problems arising in deep learning are *nonconvex*. ï¼ˆéå‡¸ï¼Œæ‰€ä»¥è§£å†³èµ·æ¥æ›´å›°éš¾ï¼‰

æ·±åº¦å­¦ä¹ ä¸­ä¼˜åŒ–é—®é¢˜ä¼šé‡åˆ°çš„å›°éš¾ï¼š

* ä¼˜åŒ–ç›®æ ‡ï¼šæ¯”èµ·ä¸€èˆ¬çš„ä¼˜åŒ–æ¥è¯´ï¼Œåœ¨è®¾è®¡Loss functionçš„æ—¶å€™è¦é¢å¤–æ³¨æ„è¿‡æ‹Ÿåˆï¼Œè¦æ³¨æ„è®¾è®¡æ³›åŒ–è¯¯å·®å’Œè®­ç»ƒè¯¯å·®

* Saddle Points å±€éƒ¨æœ€å°å€¼ï¼š
  * åˆ¤æ–­ï¼šhessençŸ©é˜µçš„æ­£å®šæ€§ï¼ˆç‰¹å¾å€¼å…¨æ­£æˆ–è€…å…¨è´Ÿèƒ½è·å¾—å±€éƒ¨æœ€å€¼ï¼Œä½†æ˜¯æœ‰æ­£æœ‰è´Ÿåˆ™å¯èƒ½è·å¾—éç‚¹ï¼›åœ¨æœ€å°åŒ–çš„é—®é¢˜ä¸­ï¼Œéœ€è¦çš„å°±æ˜¯æ­£å®šï¼Œç‰¹å¾å€¼å‡æ­£ï¼‰

* Vanishing Gradients

![image-20231025152815769](.\asset\ä¼˜åŒ–_vanishing_gradient.png)



å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼š

* éšæœºæ¢¯åº¦ä¸‹é™:
  * ![image-20231025153656780](.\asset\ä¼˜åŒ–_SGDå¼•å…¥.png)
  * ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™ï¼Œä½¿ç”¨æ‰€æœ‰çš„æ¢¯åº¦å–å¹³å‡ï¼Œè€Œæ¢¯åº¦ä¸‹é™æ˜¯éšæœºé€‰å–ä¸€ä¸ªæ–¹å‘ä½œä¸ºä»£è¡¨
  * mini-batch SGDï¼šæŠ˜ä¸­çš„åŠæ³•æ˜¯éšæœºé€‰ä¸€å°æ‰¹æ¥ç®—æ–¹å‘

* è§£å†³å±€éƒ¨æœ€å°å€¼çš„æ–¹æ³•ï¼š
  * momentum
  * ![image-20231025155122737](.\asset\ä¼˜åŒ–_momentum.png)
    * ![image-20231025155146561](.\asset\ä¼˜åŒ–_N_momentum.png)
    * ![image-20231025155216691](.\asset\ä¼˜åŒ–_N_momentum_2.png)
  * adamï¼ˆAdaptive Moment Estimationï¼‰
    * ![image-20231025161646551](.\asset\ä¼˜åŒ–_adam_1.png)
    * ![image-20231025161623422](.\asset\ä¼˜åŒ–_adam.png)
  * adamgrad
    * ![image-20231025155633509](.\asset\ä¼˜åŒ–_adamgrad.png)
  * rmsprop



Factor Scheduler???(åé¢è¿™å‡ é¡µæ²¡æ‡‚)