# 语言信息处理

资料来源：视听导课程

语言信息处理

1. NLP任务
2. 语言模型
3. 单词嵌入
4. RNN和Transformer
   4.1 RNN
   4.2 Long Short-Term MemOry RNNS (LSTMs)
       双向和多层RNN
   4.3 Transformer
       Attention
      Self-Attention
      Multi-head Self-Attention
      Transformer编码器-解码器
      Cross-attention

5.  Seq2seq
   机器翻译
   seq2segq
   Attention引l入

6. 预训练模型
   预训练的编码器和解码器
   预训练模型的使用

## 1 NLP任务

1. 机器翻译
   * 分成片段
   * 翻译效率
   * 流畅性
2. 信息提取
   * 非结构性的文本提取为数据库条目
     * 实体、关系提取
3. 问题解答
   * 不仅仅只是搜索（可能是开放的问题） ;  自然语言交互
4. 文本总结 
   * 非常依赖文本本身

 ## 2 语言模型

给定一段文本，预测下一个单词是什么: 

* 给定一段文本𝑥^{1} *,* 𝑥^{2} , … , 𝑥^{𝑡} ，我们需要计算下一个单词 𝑥^{𝑡+1} 的分布

  * $$
    𝑷( 𝒙^{(𝒕+𝟏)} | 𝒙^{(𝒕)} , … , 𝒙^{(1)} )
    $$

* 也可以把语言模型看做一个给不同的文本片段分配概率的系统，**预测下一个词**和**为文本分配概率**是等价的：

![image-20231207143405297](.\asset\语言_P.png)

1. 最经典的语言模型：**n-gram模型**

   * 一个 ”n-gram ”指**连续𝑛个词**

   * 方法：(最大似然) 统计文本中n-gram（词语组合）出现的频率，并利用这些数据来预测接下来可能出现的词语；（就是对于条件概率的展开）

     ![image-20231207143803338](.\asset\语言_markov.png)

* 增大𝑛时，n-gram的数量以指数增长，稀疏性越来越严重，通常无法让n大于5

## 3 单词嵌入

最常见的语言学意义理解方式: **指称语义学**  ，也即 symbol和idea/thing的对应

* 以前最常见的NLP解决方案: WordNet，它是包含了**同义词集和上位词**（指更一般或更广义的词）的列表

  * 缺陷：

    * 上下文敏感性：例如“proficient”是“good”的同义词。这只在某些上下文中是正确

    * 缺少单词的新含义, 需要人工对于单词理解进行修改
    * 主观性
    *  局限于表示若干种单词间的关系，但无法量化单词间的相似度

* 传统的NLP中，我们将单词视为离散的符号，这些单词的符号可以用one-hot向量表示，向量长度 = 词汇表中的单词数量
  * 但两个单词的两个向量是正交的，one-hot向量没有自然的**相似性**概念
  * **其他思路：学会在单词向量本身中编码相似性**

分布语义学：**一个词的意义由其附近频繁出现的词语所确定。**

* 一个单词的上下文是这个单词附近的词语，使用单词的上下文来构建对于该单词的解释
* 为每个单词构建一个稠密的向量，使其**与出现在相似上下文中的单词向量相似**，以向量点积来衡量相似度

* 词向量也被称为(word) embeddings或者(neural) word representations。它是一种分布式表示。

  * **Word2vec** (Mikolov et al. 2013)是一个学习词向量的框架，有两种常见形式，主要区别在于预测的方向，在本课程中我们聚焦于Skip-grams

  * **学习步骤**

    • 有一个很大的文本语料库

    • 词汇表中的每个单词都由一个向量表示

    • 遍历文本中的每个位置 𝑡，每个位置对应一个中心词 𝑐 和若干上下文词 𝑜

    • 使用 𝑐 和 𝑜 的词向量来计算给定 𝑐 时 𝑜 的条件概率

    • 调整词向量来**最大化**这些条件概率

![image-20231209104106559](.\asset\语言_word2vec.png)

* 用随机梯度下降来优化

![image-20231209104322739](.\asset\语言_word2vec2.png)

* 注意此处分母处是中心词和词库**所有单词**的相似度之和。而分子是中心词和**上下文词**的相似度计算。（这里有点像softmax函数）

* 训练结束之后将v_w, u_w的平均值作为单词w的词向量

  * $$
    𝐞𝐦𝐛_𝒘 = \frac{𝑢_𝑤 + 𝑣_𝑤}{2}
    $$


## 4 RNN 和 Transformer

### 4.1 RNN

* 可接受变长输入
* 在每个时间步上使用相同的权重
* 可以在每一个时间步上有选择地产生输出
* ![image-20231209105151986](.\asset\语言_RNN.png)

* 每一个词隐藏层，是通过上一个词的隐藏层加权和当前输入加权激活之后得到的

* 理论上可以使用许多步之前的信息，但是循环计算速度太慢，容易梯度消失/爆炸

* 训练一个RNN语言模型

  * 准备一个大的文本语料库，单词序列

  * 输入到RNN语言模型中，计算出每一步𝑡输出单词的概率分布y_hat(t)

  * 𝑡时间步下的损失函数是预测概率分布y_hat(t)与下一个实际的单词𝑦 (𝑡) 之间

    的交叉熵![image-20231209105953923](.\asset\语言_RNN_ce.png)

  * 取平均值得到训练集的总损失

  * 优化：计算整个语料库的损失和梯度在内存上太昂贵了，语料库的每一个句子可以作为一个独立的样本，使用随机梯度下降训练：计算一批次句子的损失𝐽 (𝜃) ，计算梯度并更新权重。

缺点：虽然能处理任意长度输入，但RNN难以学习并保存多个时间步的信息，隐层状态不断被重写



### 4.2 Long Short-Term Memory RNNs (LSTMs)

具体见媒认笔记

是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。

![深度学习_LSTM](.\asset\深度学习_LSTM.png)

相比RNN只有一个传递状态 h^T_t ，LSTM有两个传输状态，一个 c^t （cell state），和一个 h^t（hidden state）。（Tips：RNN中的 h^t 对于LSTM中的 c^t ）

![img](.\asset\深度学习_LSTM2.png)

以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。

LSTM的结构使得其比RNN更容易在多个时间步上保存信息

*  例如，如果单元状态中的某一个元素对应位置的遗忘门(无关上下文地)恒为1，输入门恒为0，那么该位置的信息将被无限期保留。
* 一个普通的R**NN通常难以调整其权重矩阵𝑊ℎ**来维持长时间跨度的信息，在实际中，LSTM可在大约100个时间步的跨度内保留信息，而RNN仅能维持大约7个时间步

也有其他方法可以为长距离依赖创建更加直接的连接：

#### **双向和多层RNN**

RNN已经在一个维度上“非常深了”(在时间上展开)

* 我们也可以通过使用多个RNN使它们在另一个维度上“非常深”——多层RNN（在隐藏层的深度上），这使得网络可以计算更复杂的表示

* 较低层的RNN应该计算较低层次的特征，而较高层的RNN应该计算较高层次的特征。多层RNN也称为堆叠RNN。

*  基于Transformer的网络（例如BERT）通常具有更深的层数，比如12层或24层。Transformer同样具有很多跳跃式的连接。

### 4.3 Transformer

* **线性交互距离**：RNN是“从左到右”展开的，造成了一种局部的依赖关系：缺陷是需要O(sequence length)步才能使距离很远的单词对产生相互作用。（线性交互距离）

* 前向和后向传播有**O(sequence length)**的**不可并行操作**，GPU可以同时执行众多独立的计算；但是，在过去的RNN隐藏状态被算出来之前，未来的RNN隐藏状态是无法被完整计算出来的，实际上只能实现串行计算

#### Attention

* Attention将每个单词的表示形式视为一个查询（query），用以访问和整合一组值（values）中的信息。
*  在这种机制中，不可并行化操作的数量并不会随着序列长度的增加而增加。最大交互距离：O(1)，因为任意两个单词都有相互作用
* 可以近似地将**attention**看作是在key-value的存储空间中进行模糊查找![image-20231209111526576](.\asset\语言_attention_lookup.png)

#### Self-Attention

来自相同语句的keys，queries，values

* 编码序列顺序问题的解决：使用位置表示向量（正弦位置表示：绝对位置不重要，考虑周期性，有一定的外推性质）

* 预测下一个词的信息的时候要避免信息泄露：通过将注意力权重人为设置为0来屏蔽未来信息
* 对每个Self Attention输出应用相同的前馈网络来解决引入非线性的问题

#### **Multi-head Self-Attention**![image-20231209112324047](.\asset\语言_multi_head.png)

 **Residual Connections**

**Layer Normalization**

#### Transformer编码器-解码器

* 在Transformer编码器-解码器结构中，通常用**双向模型**处理源语句，用**单向模型**生成目标输出。这是一种seq2seq的范式
  * 编码器是常规Transformer结构，但解码器被修改为对编码器的输出执行**cross-attention**操作

  * 解码器将编码器的输出作为额外的输入，来帮助解码器关注输入和输出序列之间的对应关系。这有助于模型更好地在生成过程中对输入序列进行对齐。

* Transformer**解码器**被限制为**单向**的上下文，就像语言模型那样。如果我们想要**双向**的上下文，就像双向RNN呢? 这就是Transformer**编码器**。唯一的区别是去掉了Self-Attention中的**掩码(masking)**操作

#### Cross-attention

* Self-attention: keys, queries和values是同一个来源

* cross attention:

  * 设ℎ1, … , ℎ𝑛为Transformer**编码器**的**输出**向量，设z1, … , 𝑧𝑛为Transformer**解码器**的**输入**向量
    $$
    从编码器(类似memory)中得到key和value： 𝑘_𝑖 = 𝑊_𝑘*ℎ_𝑖 ,𝑣_𝑖 = 𝑊_𝑣*ℎ_𝑖\\
    从解码器中得到query：𝑞_𝑖 = 𝑊_𝑞*𝑧_𝑖
    $$

  * 直观理解这样设计的目的：解码器需要在输入序列中做**查询**, 确定输入序列的哪些部分和当前解码器生成的内容相关


 另外的资料

RNN的运算很难平行化，尝试用CNN取代RNN，叠加多个conv layer，感受野会不断增加，相当于RNN获取的long-range dependency。但是CNN的第一层（小卷积核）无法直接获取全局信息，因此self-attention横空出世，用来解决这个问题。**self-attention**模块类似于bidirectional RNN，也可以输入一个sequence，输出一个sequence，如下图所示，每个输出都考虑了整个输入sequence的资讯，且输出sequence中 b1 到 b4 可以同时计算

![img](D:\My_desktop\Blog备份\Advanced_learning_note\asset\深度学习_自注意力.png)



Positional Encoding

即加入一个位置编码 e^i ，是一个vector，使用加法的原因是以下矩阵乘法：

![img](D:\My_desktop\Blog备份\Advanced_learning_note\asset\深度学习_position_coding.png)

unsupervised learning & transfer learning

## 5 Seq2seq

#### 机器翻译

* 用于机器翻译上，从数据中学习一个翻译模型 𝑃(𝑥|𝑦)
  * ![image-20231213145711772](.\asset\语言_翻译.png)
* 对齐是指在翻译的两个句子中特定单词的对应关系，可以是多对一，多对多

#### seq2seq

* 引入新型神经网络架构seq2seq； 神经网络机器翻译 (Neural Machine Translation, NMT) 使用单个神经网络进行机器翻译的一种方式
* Sequence-to-sequence 模型是条件语言模型 (Conditional Language Model) 的应用
  *  **语言模型 Language Model** 解码器总是在预测下一个单词
  *  **条件 Conditional** 预测是建立在源句子 𝑥 的条件上
* Encoder-decoder结构：encoder RNN, decoder RNN
  * Encoder 提取源序列的特征
  * Decoder 用于建模条件语言模型，在给定源序列的条件下生成目标序列
  * Encoder RNN的**最终隐状态**，将用于**初始化**Decoder RNN的隐状态
* 使用**teacher forcing**技术——预测下一个单词时，使用真实的前一个词作为输入。
* 自然的想法：

  * 贪心解码：在**每个时间步选择概率最高**的单词作为输出，并将其作为下一步的输入，该过程持续进行直到序列结束。

    * 问题在于翻译错了之后不能回到之前的步

  * 穷举算法：问题在于复杂度太高

  * Beam search decoding：核心思想: 在解码的每一个时间步，**跟踪 𝑘 个**最可能的分支翻译（称为**候选路径**）

    * 𝑘 即 beam size 

    * 每一条候选路径 𝑦1, … , 𝑦𝑡 拥有一个对应的分数，即其对应概率的对数

    * 而在 beam search decoding 中，不同的候选路径可能在不同的时间步产生 <END> 标记。当一条候选路径产生 <END> 时，该路径就结束了，beam search 将会继续沿着其他候选路径往前走。

    * beam search 一直持续到以下两种情况出现其一为止：
      * 达到时间步 𝑇 （ 𝑇 是预设的截止时间）
    
      * 至少 𝑛 个假设路径已经结束（ 𝑛 是预设的截止条件）
    
      * 最后的选择：越长的候选路径得分越低，选择得分最高的那一个
    
    * ![image-20231213151401289](.\asset\语言_beam_searching.png)
    
    * 相比传统的统计机器翻译（SMT），神经网络机器翻译（NMT）能更好利用上下文，是端到端系统（无需单独优化子组件），且需要更少的人力劳动
    * 但要解决的问题：长文本的上下文连贯

#### Attention引入

* 核心思想：在**解码器的每个步骤中，使用与编码器的直接连接**，将注意力集中于源序列的特定部分；解决梯度消失问题、瓶颈问题，提供可解释性

  （以下是重新对attention过程的描述）

* **编码器RNN对输入提取特征**，得到隐藏状态 ℎ1, … , ℎ𝑁 ∈ ℝℎ；

* 在每个时间步 𝑡 上, 解码器RNN产生一个隐藏状态 𝑠𝑡 ∈ ℝℎ ， 它由𝑠𝑡−1和前一个**解码输出**𝑦𝑡−1计算得来

  * $$
    𝑠_𝑡 = 𝑓( 𝑠_{𝑡−1}, 𝑦_{𝑡−1}),
    $$

* 注意力分数 𝑒𝑡 ,
  $$
  𝑒_𝑡 = [𝑠_𝑡^𝑇 ℎ_1, … , 𝑠_𝑡^𝑇  ℎ_𝑁] ∈ ℝ
  $$

* 通过 softmax 得到这一步的注意力分布 𝛼_𝑡（表明了在当前步解码时，对各个输入元素的关注程度）

  * $$
    𝛼_𝑡 = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑒_𝑡) ∈ ℝ
    $$

* 使用 𝛼𝑡 对编码器隐藏状态进行加权求和，得到注意力输出：

  * $$
    𝛼_𝑡 = \sum_{𝑖=1}^𝑁 𝛼_𝑖 ^𝑡 ℎ_𝑖 ∈ ℝ^ℎ
    $$

* 最后，将注意力输出 𝑎𝑡 与解码器隐藏状态 𝑠𝑡 并起来，然后接上线性分类层用于输出结果𝑦𝑡   𝑦𝑡 = Softmax (𝑔(𝛼𝑡 , 𝑠𝑡)) , 𝑔 是一个非线性层

* ![image-20231213152838971](.\asset\语言_seq2seq_attention.png)

## 6 预训练模型

* 预训练词库中所有词的嵌入表示 (没有上下文)，一些难以解决的问题:

  * 下游任务(例如问答)可能涉及特定的术语或表达方式，但这在预训练的语料库中可能并不常见

  * 另外，网络中的大多数参数都将是随机初始化的，而在现代的NLP实践中
    * NLP网络中的所有（或者几乎所有）的参数都是通过预训练来初始化.
    * 比如预训练方法可以从模型中隐藏部分输入，并训练模型来重建这些部分


* 预训练和微调的优势：

  * $$
    预训练提供一个好的起始估计--预训练损失：\space min_\theta L_{pretrain}(\widehat{\theta})
    $$

  *  $$
    然后从起始估计--微调损失:\space min_\theta L_{finetune}(\theta)
    $$

  *  预训练可能很重要，因为微调时，在随机梯度下降的作用下，参数将保持在theta_hat 附近——微调时得到的局部极小值点泛化得较好

* 语言模型的预训练

  * 编码器获得了双向的上下文信息，因此无法进行语言建模。
  * 想法：用特殊的[MASK]标记替换输入中的部分单词，然后预测这些单词
  * 只从被“掩盖”的词计算损失：如果x_hat 是 𝑥 做掩盖之后的版本，我们学习的是 p_theta(x|x_hat) , 这称作 Masked Language Model (Masked LM).


#### 预训练的编码器和解码器

* **encoder: BERT (transformer双向编码器)**
  * 获取双向上下文 – 能够以未来为条件；     基于Masked LM的思想
  * **预训练编码器**不完全能解决所有事情，如果任务涉及生成序列，需要使用**预训练解码器**
  * BERT和其他预训练编码器不能自然地适应自回归（一次一个单词）生成方法。
  * 是输入文本之后用于理解（编码的意思是指形成对输入的语言序列的内在关系理解；解码是指通过掌握内在关系<训练好的语言模型>来生成语言序列）


* **decoders: GPT系列**
  * GPT-1的预训练目标即为学习语言建模——给定一段文本，预测下一个单词是什么，也即计算𝑷( 𝒙 (𝒕+𝟏) **|** 𝒙 (𝒕) **,** … , 𝒙 (𝟏) )的分布，完成这种任系统叫做语言模型；为了针对特定任务微调，需要引入**特殊token**，设计输入格式
  * GPT-2 是 GPT 的更大版本，得到了相当大的性能提升
    * GPT-2 通过**大规模无监督学习**来提高模型的泛化能力,引入了**提示(prompt)**的概念，完全以自然语言指令描述任务，不再为任务设计特殊token, GPT-2 即使没有进行过特定任务的微调，也能够通过任务提示来理解并执行任务
    * GPT-3同样没有改变模型结构，但参数量由GPT-2的1.5B增加至175B，数据量由40G增加至约45TB(>1000倍提高！）


#### 预训练模型的使用

* 到目前为止，我们介绍了使用预训练模型的两种办法 :
  *  从模型确定的分布中采样 (可能提供零样本的prompt)
  * 针对我们关心的任务进行微调

**指令微调（Instruction tuning）**

* 假设有很多NLP任务，研究人员针对每个NLP任务设计多种提示模板作为指令，并使用这些指令以及相应的数据对模型进行微调。
  * 微调（Fine-tuning）：需要尽可能多的任务相关的数据；每一个任务需要单独训练一个模型

* 训练完成后，给大语言模型一个它从未见过的全新任务的指令，即让大语言解决零样本(zero-shot)任务。零样本上任务上的表现体现了模型的泛化能力
  * **零样本提示（Zero-shot Prompting）：**直接将任务描述输入模型

**人类反馈强化学习（Reinforcement Learning from Human Feedback）**

* 三个阶段：有监督微调 -> 奖励模型 -> 策略优化
* 主要特点：更面向人类用户真实需求。Prompt从大量用户提交的真实请求中抽样而来，而不是固定好研究任务的范围让研究人员给出任务描述

让大语言模型会发和解决更复杂的问题——

**思维链**

思维链必须在模型规模足够大时才能涌现，思维链的应用领域有限

* **零样本思维链（Zero-shot Chain of Thought）:**LLM本身是具备推理能力，需要通过合适的提示语来让LLM释放潜力。加“Let's think step by step”这句提示语，让LLM会输出具体的推理过程

* **少样本思维链（Few-shot Chain of Thought）:**为了教会LLM模型学会推理，可以给出一些人工写好的推理示例。

* **自洽性（Self-Consistency）:**自洽性是对 CoT 的一个补充，它多次生成推理链条，然后取多数答案作为最终答案

* **最少到最多提示（Least-to-most prompting, LtM）:**最少到最多提示过程将思维链提示过程进一步发展，首先将问题分解为子问题，然后逐个解决。

**低秩自适应（Low-Rank Adaptation）：**

* 核心思想：在原始预训练语言模型（PLM）的某些层中，插入低秩矩阵来微调模型。

* 选择 需要修改的权重矩阵，例如某层注意力的W𝑄, 𝑊𝐾, 𝑊𝑉矩阵，将选择的权重矩阵替换为两个低秩矩阵的乘积𝐴, 𝐵；微调时只训练降维矩阵𝐴与升维矩阵𝐵，只需微调少量参数

**适配器（Adapter）：**

* 在预训练模型每一层(或某些层)中添加Adapter模块
* 微调时**冻结预训练模型主体**，由Adapter模块学习特定下游任务的知识
*  Adapter进行下投影和上投影，可以看做一种压缩-复原结构

**前缀微调（Prefix-tunning）：**

* 前缀微调将一个连续的特定于任务的**向量序列添加到输入**
* 与提示不同的是，前缀完全由自由参数组成；对每个额外任务，只产生非常小的开销
