# 听觉信息处理

资料来源：视听导课程

1. 听觉信息处理概述

   1.1 机器听觉任务
      自动语音识别(Automatic Speech Recognition, ASR)
      自动语音翻译(Automatic Speech Translation,AST)
      语音增强(Speech Enhancement)
      语音分离(Speech Separation)
      说话人识别(Speaker Recognition)
      说话人日志化(Speaker Diarisation)
      音频事件检测(Audio/Sound Event Detection)
      动情感识别(Automatic Emotion Recognition, AER)
   1.2 科学机制
      语音和语音链
      语音的表示

2. 语音识别基础
   2.1 隐式马尔可夫模型
   2.2 前后向过程
   2.3 Viterbi算法
   2.4 对词和句子建模
          训练
   2.5 观测密度函数
          扩展观测
   2.6 文本归一化与词错误率
         2.6.1 连接主义时序分类(CTC)方法
                算法介绍
                CTC VS NN-HMM
                Lattice-Free MMI训练(LF-MMI)
         2.6.2 端到端的方法
                端到端的CTC
                循环神经网络变换器(Recurrent Neural Network Transducer，RNN-T)

   ​             基于注意力机制等(字列)编码器和解码器(Attention-based Encoder.Decoder，AED)
   ​             连续累积和激发(CIF)方法
   ​              评价

## 1 听觉信息处理概述

### 1.1 机器听觉任务

##### 自动语音识别（Automatic Speech Recognition, ASR）

* 输入语音序列转换为与语音内容对应的文本序列，有时也称为语音转文字

  （Speech-to-Text, STT），是语音处理任务的核心

* 统计语音识别（Statistical ASR）的基本公式

  * $$
    W^*=argmax_{W}P(W|O)\\
    =argmax_{W}P(O|W)P(W)/P(O)\\
    =argmax_{W}P(O|W)P(W) 
    $$

  * W是输出词串，O是输入音频，P(O|W)是声学模型，P(W)是语言模型


* **（输入语音）-->声学模型（转为音素）-->发音模型（转为词）-->语言模型-->搜索解码（输出词串）**
  * ![image-20240115173540522](.\asset\语音pipeline.png)

* Whisper-v3模型
  * 基于1百万小时（100年=87.5万小时），语音数据训练
  * 支持97种语言的语音识别
  * 模型同时支持句子端点检测和语音翻译
  * 部署在OpenAI手机端ChatGPT服务中

* Google的Universal Speech Model（USM）模型
  * 使用1000万小时无标注语音预训练，使用200万小时有标注语音精调
  * 模型最大2B参数（20B），300种语言多语言ASR

##### 自动语音翻译（Automatic Speech Translation, AST）

把输入语音序列根据内容转换为另一种语言的对应对应文本或语音

流式/非流式：同声/交替传译（Simultaneous/Consecutive Interpretation）

* Meta SeamlessStreaming
  * 同时支持流式语音-语音（同声传译）和语音-文本翻译
  * 使用单调注意力机制和流式调优方法，将已训练的非流式模型（SeamlessM4T v2）调优为**增量式音频输入**和**部分文本输出**

##### 语音增强（Speech Enhancement）

* 狭义上通常指“**降噪**”和“**去混响**”
* 可以使用ASR等下游任务性能作为客观指标
* 尚无公认可以一致提升下游ASR性能的语音增强方法

##### 语音分离（Speech Separation）

声源位置（多麦克风阵列）和视觉有助于语音分离，但常研究单麦克风的语音分离

* 深度聚类方法（时间-频率桶）
* 排列不变性训练（Permutation Invariant Training, PIT）
* 目标说话人提取（Target Speaker Extraction）

##### 说话人识别（Speaker Recognition）

* **开集合**分类问题
  * 说话人**辨认**（从多个人中选一人）vs 说话人**确认**（确认是否本人）

* 是否与内容有关
  * **文本相关**（限制语音内容）vs **文本无关**（不限制语音内容）

* 精确度通常低于人脸识别，但可控性更强

##### 说话人日志化（Speaker Diarisation）

* 在长音频流中找出“**谁**在**何时**说话了”的任务
* 说话人日志化传统上也称为“说话人分割和聚类”，通常以流水线方式实现

##### 音频事件检测（Audio/Sound Event Detection）

* 检测并分类音频事件的任务

##### 自动情感识别（Automatic Emotion Recognition, AER）

### 1.2 科学机制

##### 语音和语音链

![image-20240115180131994](.\asset\语音链.png)

##### 语音的表示

* 语音波形（Waveform）：麦克风采集的代表声音压力信号波形的电信号
  * 一维信号
  * 高频
  * 时序
  * 变长

* 语音波形是**非平稳**的，并且包含**伪周期**和**随机分量**的混合
* 语音可视为一串**音素**（Phone）的组合，音素是语音中任何可区分的声音，改变音素不一定改变词语
* **音位**（Phoneme）是语言中区别词语的最小语音单位，**改变音位会改变词**，音位不是实际发音
  * 一个音位的不同音素实现称为**同位音**（Allophone）
  * 英语通常分为40个音素，包括**元音**和**辅音**
  * 普通话的基本单位是**声韵母**（Initial/Final），有**零声母**（Zero Initials），每个韵母是1-3个元音的**复合元**音

*  **国际音标**（International Phonetic Alphabet，IPA）
  * 用来文本化表示任何语言的音素和音位，但实践中发现远不足以表示语音的多样性
  * 语音无法彻底文本化

* 浊音（Voiced）
  * 大多数元（**/iy/**）音和部分辅音（**/z/**）
  * 空气通过肺部产生震动
  * “f**ee**l”、“h**i**t”等元音，在**音高**（Pitch）频率上有准周期性（Quasi-Periodic）

* **基频**（Fundamental Frequency, F0）：发声时频率最低的正弦波的频率，
  * 音高是对基频的感知（个体对F0的主观解释），可以认为语音中的音高感知只与基频有关

* 清音（Unvoiced）
  * 部分辅音（**/s/**），耳语及某些语言有清元音

* **傅立叶变换**（Fourier Transform）实现了在时域和频域之间的转换
  * 将语音分为**10ms**长的段（**短时平稳假设**）
  * 语音的**包络**（Envelope）反应音素和音节转换对应的能量变化，是刻画不同发音的关键



## 2. 语音识别基础

![image-20240115215619421](.\asset\pipeline.png)

### 2.1 隐式马尔可夫模型

输入语音序列：长度为T 的语音帧序列 O = o1o2… oT（观测序列）

*  马尔可夫模型（Markov Model）
  * 包含 N个**离散状态**1, 2, …, N ，及状态间可能的转移
  *  （一阶）马尔可夫性：下次状态转移只与当前状态有关，与过去状态无关（序列/串珠模型）
  * 状态**转移概率** a_ij：状态 i 到 j 间的转移概率，通常假设符合几何分布
  * **发射**（Emitting）状态：消耗序列中一个观测（语音帧）计算观测概率b_i(o_t)的状态
  * **非发射**（Non-Emitting）状态：不消耗观测的特殊状态
  * 状态序列（观测与状态间的**对齐**）Q= q1q2…qT，其中q_t ∈ {1,2, …, N}

* 一阶马尔科夫状态转移+假设每时刻观测独立生成

  * $$
    p(O,Q|\lambda)=P(Q|\lambda)p(O|Q,\lambda)=\Pi_{t=1}^TP(q_t|q_{t-1})p(o_t|q_t,\lambda)=\Pi_{t=1}^T a_{q_{t-1}q_{t}}b_{q_t}(o_t)\\
    其中a_{ij}是i\rightarrow j的状态转移概率，b_i(.)是状态i的发射概率
    $$

  * 也即利用马尔科夫性，序列行为转化为每一步状态转移和每一步状态输出概率的直接相乘

* 隐式马尔科夫模型

  * **“隐藏”**（Hidden）: 不能直接观察到的状态序列，观测和状态对齐未知（状态序列Q隐藏）

  * 训练：

    * $$
      考虑所有的Q，p(Q|\lambda)=\sum_Q p(O,Q|\lambda)
      $$

  * 前向后向过程：通过递推有效计算所有Q的概率

  * 测试：找出最可能的Q

  * Viterbi算法：

    * $$
      找出argmax_Q p(O,Q|\lambda)
      $$

  * HMM**训练**有时也可使用Viterbi搜索，先获得帧和状态间的固定对齐

    * 下面这个式子不是很懂，为什么约等于最优隐层状态下的发射概率？（怎么有点像最可几方法？）

    * $$
      p(O|\lambda)\approx argmax_Q p(O,Q|\lambda)
      $$

* * 

  * 

### 2.2 前后向过程

* 前向概率：

  * 计算前向过程所需的前向概率

  * $$
    输出序列为o_{1:t} ，且t时刻状态为j的概率：\\\alpha_j(t)=p(o_{1:t},q_t=j|\lambda)
    $$

  * 全概率公式+动态规划

  * $$
    [1]\space p(o_{1:t}|\lambda)=\sum_{j=1}^N \alpha_j(t)\\
    p(o_{1:t},q_t=j,q_{t-1}=k|\lambda)=\alpha_k(t-1)\space a_{ij}\space b_j(o_{t})\\
    [2]\space \alpha_j(t)=b_j(o_{t})[\sum_{k=1}^{N-1}\alpha_k(t-1)a_{kj}]
    $$

* 后向概率

  * 计算后向过程的后向概率

  * $$
    后向概率：在t时刻状态为j时输出为o_{t+1:T}的概率\\
    \beta_j(t)=p(o_{t+1:T}|q_t=j,\lambda)
    $$

  * 全概率公式+动态规划

  * $$
    [1]\space p(O|\lambda)=\sum_{k=2}^{N-1}p(O,q_1=k|\lambda)\\
    =\sum_{k=2}^{N-1}p(O|q_1=k,\lambda)P(q_1=k|\lambda)\\
    =\sum_{k=2}^{N-1}a_{1k}b_k(o_1)\beta_k(1) \\
    [2]\space \beta_j(t)=\sum_{k=2}^{N-1}a_{jk}b_k(o_{t+1})\beta_k(t+1)
    $$

* 前向后向过程

  * 在序列处理中可以定义**状态占有率**（State Occupancy）

  * $$
    状态占有率：t时刻状态为j的后验概率\\
    \gamma_j(t)=P(q_t=j|O,\lambda)
    $$

  * 利用前向概率和后向概率计算状态占有率

  * $$
    [1]\space \gamma_i(t)=P(q_t=i|O,\lambda)=\frac{p(O,q_t=i|\lambda)}{p(O|\lambda)}\\
    [2]\space \alpha_j(t)\beta_j(t)=p(o_{1:t},q_t=j|\lambda)p(o_{t+1:T}|q_t=j,\lambda)\\
    =p(o_{1:t}|q_t=j,\lambda)p(o_{t+1:T}|q_t=j,\lambda)P(q_t=j,\lambda)\\
    =p(O|q_t=j,\lambda)P(q_t=j,\lambda)=p(O,q_t=j|\lambda)\\
    [3]p(O|\lambda)=\sum_{i=1}^{N}\alpha_i(t)\beta_i(t)
    $$

  * 前向后向过程计算复杂度正比于T*N^2

    * 实际用剪枝策略只计算子空间；另外为避免溢出，常使用对数运算

### 2.3 Viterbi算法

Viterbi搜索 vs 前向后向过程

*  帧同步的Viterbi搜索

  * **近似似然**计算

  *  只进行**从左到右**的计算

    *  **较省内存和计算**，容易实现束搜索等

    * 是从左到右的广度优先算法，**可以用于流式搜索**

  * 可以用于**训练**或**测试解码**
    * Token Pass Model：将状态转移看作Token传递，在每个时刻丢弃概率过小的Token

  * Viterbi给出唯一的“最优”分段概率值为0-1分布

* 前向后向过程

  * **准确似然**计算
  *  **计算和内存开销较大**，需要**从左到右**和**从右到左**的计算，**无法用于流式搜索**
  *  通常只用于训练

  * 前向-后向过程 软对齐

### 2.4 对词和句子建模

因词汇过多，使用**子词**而非词构建声学模型

* **降低**模型存储和计算**复杂度**（词数过多）
* **缓解数据稀疏**问题
  * 可以**共享**不同词汇的**数据**训练相同子词模型
  * **解决**为无训练样本的**集外词**（Out-of-Vocabulary，OOV）建模的问题

* 为每个子词分别构建一个HMM，可以使用音素作为子词

**复合**（Composite）HMM

*  通过将多个HMM的非发射状态合并，可以构造复合HMM
  * 子词->词（可能存在多发音）
  *  词->句子

* 可直接使用原始HMM的所有算法

![image-20240116003259298](.\asset\复合HMM.png)

![image-20240116003233621](.\asset\句子建模1.png)

**对齐**句子中的语音和非语音

*  在句首和句尾加入 **sil** 模型（检测长非语音）
* 在句中每个词后加入 **sp** 模型（检测可能出现的停顿）

强制对准（Forced Alignment）技术：P2FA（HTK）、MFA（Kaldi）

* 给定词语内容，找出最可能的**音素序列**
*  找出每个音素的**起止时间**
* 可视为一种**受（词语内容）约束**的搜索

![image-20240116003608300](.\asset\相关音子模型.png)

**其他常用子词单元**

* 语素（Grapheme）
  *  如拉丁字母、汉字等构成书写系统的基本单元
  *  可对语素进行上下文相关建模，如：state = **/s+t/ /s-t+a/ /t-a+t/ /a-t+e/ /t-e/**

*  UTF-8（Unicode Transformation Format – 8 bit）字节
  * 任何Unicode编码可拆分为**1-4个UTF-8字节**（**256个子词**），适合Unicode的159种书写系统

*  词片（Word Piece Model，WPM）和字节对编码（Byte Pair Encoding，BPE）
  * 根据词片出现的频率，将词切分词片

* 句片（Sentence Piece Model，SPM）
  * Google重新实现并开源的**跨词WPM**单元



##### 训练

训练 HMM 声学模型，需要什么样的数据标注

* 词语文本标注
* 每个词语或音素的时间标注（对语音进行预分段）

无需初始时间标注的 Flat-Start 训练

* 对所有HMM采用**相同随机初始化**，等价于对语音进行**等长分段**
* （因语音按时序标注）大多数HMM能**大致**与对应音频段**匹配**
* 迭代更新HMM，得到比随机初始化**更好**的参数
*  用更新的模型再次分段，得到**更好**的音频与文本对应，再次训练
* 迭代直到模型**收敛**

### 2.5 观测密度函数

**HMM的观测概率密度函数**

有两个方法将连续观测序列和HMM结合：

- 将连续观测**聚类为离散类别**,这就对应到HMM离散观测序列. 然后按照HMM离散观测序列处理.

- 更改**观测概率函数为密度函数**,例如高斯概率密度, 这需要替换观测状态序列生成函数.

  * 常用高斯混合模型（Gaussian Mixture Model，GMM)

  * $$
    p(o_t|q_t=i,\lambda)=\sum_{m=1}^{M}P(m|q_t=i,\lambda)p(o_t|q_t=i,\lambda)
    $$

  * GMM 是一种**函数近似器**，能构成**任意形状**概率密度函数的光滑近似；通常给每个HMM状态关联一个独立的GMM

  * 为避免估计过多的协方差参数，常使用**协方差矩阵为对角阵**的GMM

$$
b_i(o_t)=\sum_{m=1}^{M}\omega_{im}N(o_t|\mu_{im},\sigma_{im}^2)\\
N为高斯分布,\omega_{im}为状态i的GMM第m个高斯的权重
$$

http://www.shizhuolin.com/2021/12/16/3727.html

传统上，通常使用**Baum-Welch算法**训练HMM

* 前向后向过程计算**状态占有率**
* 根据状态占有率，使用**EM（Expectation Maximisation）算法**迭代更新模型参数
* **全batch**更新一次模型，EM可看作一种二阶优化算法

简化起见，考虑使用梯度下降训练HMM【remain】

![image-20240116010114724](C:\Users\zzt\AppData\Roaming\Typora\typora-user-images\image-20240116010114724.png)
$$
d
$$

##### 扩展观测

HMM假设每个观测独立生成

* 扩展发射概率密度，用更多输入观测缓解这一假设(???????为什么要缓解这一假设)

  * $$
    b_i(o_t)=b_i([x_{t-2},x_{t-1},x_{t},x_{t+1},x_{t+2}])\\
    x_t是t时刻的声学特征向量；o_t是观测向量;\Theta是差分特征的计算窗宽（超参）
    $$

* 受对角协方差矩阵的GMM限制，输入观测向量的各维度须线性无关

通过时间差分特征扩展观测

* 差分特征

  * $$
    \Delta x_t = \frac{\sum_{\theta=1}^{\Theta} (x_{t+\theta}-x_{t-\theta})\theta}{2\sum_{\theta=1}^{\Theta}\theta^2}
    $$

  * 差分特征与特征线性独立，故 扩展的差分特征

    * $$
      o_t=[x_t, \Delta x_t]^T\\
      可以基于这种方法计算\Delta \Delta x_t等，构建更多的扩展特征
      $$

 

* 使用人工神经网络模型（Artificial Neural Network，ANN）扩展GMM输入

  * ANN 模型对其输入特征**无任何假设**，可取其**隐层特征**作为GMM的输入观测
  * **Tandem 方法**（级联的两个系统）
    * 用ANN训练单音子/三音子分类器，将ANN估计的音子概率降维后输入GMM

  * Bottleneck（瓶颈层）方法
    *  ANN分类器带有窄的隐藏层（瓶颈层），将瓶颈层特征输入GMM

* 直接使用 ANN 估计 HMM 的观测概率密度函数

  *  使用一个 ANN 音素分类器取代 GMM 的**Hybrid（杂交）方法**

  *  可直接**堆叠任意多帧输入特征**，可在所有HMM状态间**共享** ANN **隐藏层参数**

  * 用贝叶斯公式将ANN输出后验概率转为似然

$$
ln\space p(o_t|i)=ln\space P(i|o_t)+ln\space p(o_t)-ln\space P(i)\\
ln\space P(i)=ln\space count(i)-ln\sum_{i^{'}}count(i^{'})\\
count(i)是状态i在训练数据中的帧数
$$

* 直接使用 DNN 估计 HMM 的观测概率密度函数

  *  DNN：早期定义为**超过4个隐藏层**的ANN
  * 使用 DNN 构造**基于帧的三音子共享状态分类器**
    * 可在Hybrid方法中使用其他任何深度学习模型，如RNN/LSTM、TDNN、CNN、Transformer、Conformer等构建 HMM 声学模型

  * 可使用前向后向过程训练，但难度较高
    * 实践中常先用预训练的 GMM-HMM 获得**对齐**
    * 根据对齐结果获得每个**语音帧的标签**（共享状态）
    * 在一个Minibatch中，在**帧层级随机采样数据**，可以更好的平均说话人个性、信道、噪声等，从而能更好的估计随机梯度

（疑问：hybrid方法？为什么要扩展输入/特征？前面为什么要打破观测独立的假设？为什么GMM-HMM就是无时间标注的？）

### 2.6 文本归一化与词错误率

* Hybrid方法：ANN-HMM系统通常需要先获得**时间对准（人工或自动）**
  *  希望ANN像GMM-HMM一样从无时间点信息的标注开始训练

#### 2.6.1 连接主义时序分类（CTC）方法

* 避开输入与输出手动对齐的一种方式

##### **算法介绍**

给定输入 X ，CTC输出每个可能输出及其**条件概率**（后验概率）。问题的关键是CTC的输出概率是如何考虑 X 和 Y 之间的对齐的，这种对齐也是构建损失函数的基础。（这里的X对应输入O，Y对应输出W）

1. 首先我们分析CTC的**对齐**方式

   * 如果将输入 X 分割成若干个时间片，每个时间片得到一个输出，一个最简的解决方案是合并连续重复出现的字母，但是会出现的问题：
     * 不是所有都能对应，比如间隔/停顿

     * 对于重复字符的情况会被误合并


   * 解决办法
     * 引入输出空字符（\epsilin）和子词（早期为音子)，基于字典将词标注转为子词标注（例如字母）；在相邻的相同子词间插入空字符，如hello->hel \epsilin lo，每帧输出一个子词


2.  然后我们分析CTC的**损失函数**的构造

   * 对应标签 Y ，其关于输入 X 的后验概率可以表示为所有映射为 Y 的路径之和，我们的目标就是最大化 Y 关于 x=y 的后验概率 P(Y|X) 。假设每个时间片的输出是相互独立的，则路径的后验概率是每个时间片概率的累积，公式及其详细含义如下式：
     $$
     F_{CTC}=ln \sum_{Q\in A}\Pi_{t=1}^{T}P(q_t|O,\lambda)\\
     阶乘部分是每一个对其的step\space by\space step的概率，求和是对所有对齐求和
     $$

   * A为输出字符串与帧间的所有对齐

   * 直接建模P(W|O)后验概率 而非似然P(O|W)
     * 语言模型融合不再符合贝叶斯公式
     * 统计每个输出单元的先验概率P(q_t|\lambda) ，利用贝叶斯公式将P(q_t|O,\lambda)后验概率转化为p(O|q_t,\lambda)似然，可提高语言模型融合效果
     * 抵消空字符 \empsilin 的先验概率可改善CTC输出分布尖锐而置信度和时间对齐差的问题 

   * 训练时使用前向后向过程计算音频和子词间的软对齐；CTC的训练数据组织：随机采样句子，补零到等长

3. 预测

   * 训练好一个RNN模型时，给定一个输入序列 X ，我们需要找到最可能的输出，也就是求解:

   * $$
     Y^*=argmax_Y \space(Y|X)
     $$

   * 求解最可能的输出有两种方案，一种是Greedy Search，第二种是beam search

     *  Beam size=1的贪心解码可能导致概率计算出现较大误差
       * Beam Search是寻找全局最优值和Greedy Search在查找时间和模型精度的一个折中；一个简单的beam search在每个时间片计算所有可能假设的概率，并从中选出最高的几个作为一组。然后再从这组假设的基础上产生概率最高的几个作为一组假设，依次进行，直到达到最后一个时间片。

     * 测试使用Viterbi搜索，结果相邻相同子词去重
       * 在Viterbi搜索中，根据输出格式的字符串（去重并移除\empsilin）进行状态合并（概率计算、排序和剪枝），对每个输出字符串，根据是否以\empsilin 结尾，分别对应保留两个搜索状态

##### CTC **vs** NN-HMM

* CTC
  * CTC等价于特殊的HMM
    * 方法1：或两状态HMM，其中一个状态为空字符
    * 方法2：或构造可跳过的空字符HMM插入子词间（类似sp）
  * 类似HMM，CTC也是一种帧同步（Frame Synchronous）模型
    * 解码的每一步解码算法消耗一帧
    * 每一步输出一个子词或空字符
  * 容易从**无时间标注**的数据开始训练，练过程需要计算**前向后向过程**，较耗费时间和内存
  * 当Minibatch足够大，或训练时间足够长，可以学到更好的时间对齐
  * 对状态转移缺少必要的约束，输出分布尖锐，输出置信度差、时间对齐差，**但解码速度快**
  * 可以直接复用HMM的所有解码和后处理技术

* NN-HMM
  * 容易从**有时间标注的数据**开始训练，可以在**帧层级**实现更好的数据随机化，省时间和内存
  * 输出置信度好、时间对齐能力好
  * 有专门的非语音单元，可以准确的区分语音和非语音
  * HMM结构处理、搜索解码等算法实现复杂

* 不同的训练数据组织方法
  * 帧层级随机化（利用已有时间对齐）：DNN-HMM等
  * 句子层级随机化（保留完整句子结构）：CTC等
  * 段落层级随机化（保留每个位置在相邻Minibatch中句子的连续性）：语言模型等；可使模型具有更长上下文，但比目前方法消耗更多资源

##### Lattice-Free MMI训练（LF-MMI）

* 基于MMI准则训练，相较CTC仅考虑输入语音与基准标注，MMI考虑了所有可能假设
* 为所有句子使用较小的公共的**n-gram**音子Lattice，可在GPU上并行
* 将前向后向过程组织为适合GPU的**矩阵运算**，**未使用对数，容易造成下溢出**（回忆前向后向过程）
* 将所有句子切短为1.5秒的片段，减少前向后向过程的步数，并方便多片段并行
* 每一步计算都对前向概率和后向概率重新进行归一化和校准
*  使用特殊的Leaky-HMM结构，通过以小概率允许HMM各状态间的转移，改善数值稳定性
* 类似I-Smoothing，基于对齐将MMI和交叉熵（Cross-Entropy）目标函数构造多任务训练

**端到端（End-to-End）的LF-MMI**

* 端到端：使用单一DNN模型从初识开始训练，未使用任何预训练的ASR、对齐结果和决策树聚类
* 使用未聚类的全双音子集合作为模型输出单元，也使用Grapheme进行建模，但效果较差
* 基于训练数据音子标注（词标注查字典获得）随机插入静音（sil）生成公共的n-gram音子Lattice
* 将所有句子根据长度归入30个等分的桶内，将每个句子重采样为与其长度最接近的桶长
* 效果（在学术界常用数据集上）远好于其他端到端方法



#### 2.6.2 端到端的方法

##### 端到端的CTC

* 端到端：一次训练完成学习（无需迭代系统），无字典

* 关键修改
  * 使用Grapheme建模，避免使用字典
  * 使用MBR目标函数

* **需使用语言模型**
*  缺点包括无法处理输入短于输出的情况
* 后续逐渐使用词片等建模单元，及更强大的声学模型结构（Conformer等）

##### 循环神经网络变换器（Recurrent Neural Network Transducer，RNN-T）

* 端到端：一次训练完成学习，无语言模型

*  关键修改
  * 结构上可看作联合优化**CTC声学模型和RNN语言模型**，两模型使用相同的音素输出单元（及空字符单元），并联合训练（Graves, 2012）
  * 结构上令CTC声学模型和RNN语言模型共享输出层和倒数第二层（Graves et al., 2013）
  *  扩展CTC目标函数，将前向后向过程从1D修改为2D。模型每输出空字符，消耗一帧；或输出子词，不消耗时间。允许输入序列短于输出
  *  后续逐渐使用词片等子词单元，实现无字典

* RNN-T的输出同时具有帧同步（横轴）和标签同步（纵轴）的特性
* 可以将Transformer看作连续值隐状态逐渐变长的特殊RNN，同样构造RNN-T

##### 基于**注意力机制等（序列）编码器和解码器**（Attention-based Encoder-Decoder，AED）

* 利用**注意力机制（而非前向后向过程或Viterbi搜索**）实现音频和文本间的对齐
* 是一种**标签同步模型**（每次解码输出一个标签）
* 在解码器模型中混合声学和语言信息联合建模
* 相比HMM、CTC和RNN-T，较难实现流式ASR

##### 连续累积和激发（CIF）方法
* 利用仿生学上更合理的尖峰神经网络（Spiking Neural Network）的思路，注意力机制累积超过阈值则在下一层生成一个新的输入向量
* 容易实现流式ASR、好的时间对齐
* 在中文上容易实现好的WER，在英文上较困难

* 组合CTC、RNN-T、AED等，可以实现更多端到端方法

##### 评价

没有真正“端到端” Everything is the same, just a bit simpler and better!

端到端ASR研究中：

* 特征提取： 自动特征提取与梅尔滤波器组效果相近，故通常**保留传统特征提取**

* 语言模型：进行声学和文本联合建模，但因内置语言模型文本训练数据不足，通常还**使用外置语言模型**

* 序列区分性训练：RNN-T和AED存在曝光偏置（Exposure Bias）问题，MMI/MBR训练甚至更加有效

* 说话人自适应：有充足特定说话人（特定噪声）数据时，说话人（噪声）自适应仍然有效

* 解码算法：解码器模型实现大幅简化，但多遍解码等仍然有效

* 后处理：仍然必不可